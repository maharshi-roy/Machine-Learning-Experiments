{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Neural Machine Translation using LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HelTzK5x_6q8uk0qmxzP533Di5H1hAWC",
      "authorship_tag": "ABX9TyO4xUmcM4q+igHHPAqmHVwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maharshi-roy/Machine-Learning-Experiments/blob/main/Neural_Machine_Translation_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhoLQDkGgdx8"
      },
      "source": [
        "#Make imports\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import os\n",
        "import seaborn as sns\n",
        "import string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQNeTtWZlwUc",
        "outputId": "ed452ff4-c768-4395-dcd5-85b9e8ba14ea"
      },
      "source": [
        "#TPU settings\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.6.0\n",
            "Running on TPU  ['10.76.194.114:8470']\n",
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.76.194.114:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.76.194.114:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-_bF5bilI2f"
      },
      "source": [
        "def preprocess(text):\n",
        "  text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\d','',text)\n",
        "  text = re.sub(r'\\s+',' ',text)\n",
        "  text = text.strip()\n",
        "  return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKe9ncQoZfox"
      },
      "source": [
        "#Extract dataset and preprocess\n",
        "dataset_root = \"./drive/MyDrive/ml/datasets/\"\n",
        "\n",
        "if os.path.exists(dataset_root + \"parallel/preprocessed_data.pickle\"):\n",
        "  with open(dataset_root + \"parallel/preprocessed_data.pickle\", 'rb') as f:\n",
        "    english_sentences, hindi_sentences = pickle.load(f)\n",
        "else:\n",
        "  if not os.path.exists(dataset_root + \"parallel/IITB.en-hi.en\"):\n",
        "    os.system(\"tar -xzf \" + dataset_root + \"parallel.tgz -C \" + dataset_root)\n",
        "\n",
        "  with open(dataset_root + \"parallel/IITB.en-hi.en\",'r') as f:\n",
        "    english_sentences = f.read().split('\\n')\n",
        "\n",
        "  with open(dataset_root + \"parallel/IITB.en-hi.hi\",'r') as f:\n",
        "    hindi_sentences = f.read().split('\\n')\n",
        "\n",
        "  english_sentences = [preprocess(en) for en in english_sentences]\n",
        "  hindi_sentences = ['<START> ' + re.sub('[a-zA-Z]','',preprocess(hi)) + ' <END>' for hi in hindi_sentences]\n",
        "\n",
        "  with open(dataset_root + \"parallel/preprocessed_data.pickle\",'wb') as f:\n",
        "    pickle.dump((english_sentences, hindi_sentences), f)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjnK8XArDPUe"
      },
      "source": [
        "#Some parameters\n",
        "vocab_size = 10000\n",
        "total_sentences = 100000\n",
        "maxlen = 20\n",
        "epochs = 10"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyHjYB_eMpD7"
      },
      "source": [
        "en_data = []\n",
        "hi_data = []\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "for (en,hi) in zip(english_sentences, hindi_sentences):\n",
        "  l = min(len(en.split()), len(hi.split()))\n",
        "  if l <= maxlen:\n",
        "    en_data.append(en)\n",
        "    hi_data.append(hi)\n",
        "    cnt += 1\n",
        "  if cnt == total_sentences:\n",
        "    break"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwR2rlOvB-xZ",
        "outputId": "11001bd4-41cb-44a3-e897-af3dfa6b4c5e"
      },
      "source": [
        "#Tokenize the texts and convert to sequences\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
        "en_tokenizer.fit_on_texts(en_data)\n",
        "en_sequences = en_tokenizer.texts_to_sequences(en_data)\n",
        "\n",
        "hi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>', lower=False)\n",
        "hi_tokenizer.fit_on_texts(hi_data)\n",
        "hi_sequences = hi_tokenizer.texts_to_sequences(hi_data)\n",
        "\n",
        "english_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "hindi_vocab_size = len(hi_tokenizer.word_index) + 1\n",
        "print(\"English Vocab Size: \", english_vocab_size)\n",
        "print(\"Hindi Vocab Size: \", hindi_vocab_size)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocab Size:  1967\n",
            "Hindi Vocab Size:  2046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luUa7TE6RYFq"
      },
      "source": [
        "#Prepare encoder data\n",
        "encoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(en_sequences, maxlen=maxlen, padding='post')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olIjin62TPF7"
      },
      "source": [
        "#Prepare decoder data\n",
        "decoder_inputs = []\n",
        "decoder_outputs = []\n",
        "\n",
        "for hi in hi_sequences:\n",
        "  decoder_inputs.append(hi[:-1])\n",
        "  decoder_outputs.append(hi[1:])\n",
        "\n",
        "decoder_inputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_inputs, maxlen=maxlen, padding='post')\n",
        "decoder_outputs = tf.keras.preprocessing.sequence.pad_sequences(decoder_outputs, maxlen=maxlen, padding='post')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5OWbUrPRENr",
        "outputId": "ed07bea8-e736-4f0c-e1d4-c4143926ae5b"
      },
      "source": [
        "# Training and Testing split\n",
        "# 95%, 5%\n",
        "split = int(0.95 * total_sentences)\n",
        "\n",
        "X_train = [encoder_inputs[:split], decoder_inputs[:split]]\n",
        "y_train = decoder_outputs[:split]\n",
        "\n",
        "# Test data to evaluate our NMT model using BLEU score\n",
        "X_test = en_data[split:]\n",
        "y_test = hi_data[split:]\n",
        "\n",
        "print(X_train[0].shape, X_train[1].shape, y_train.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23750, 20) (23750, 20) (23750, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUboXx8WjDnP",
        "outputId": "fe9b8314-2b17-4c34-de6d-ba1eadc46a0a"
      },
      "source": [
        "#Define LSTM model\n",
        "d_model = 256\n",
        "\n",
        "#Encoder\n",
        "inputs = tf.keras.layers.Input(shape=(None,))\n",
        "x = tf.keras.layers.Embedding(english_vocab_size, d_model, mask_zero=True)(inputs)\n",
        "x, state_h, state_c = tf.keras.layers.LSTM(d_model,activation='relu',return_state=True)(x)\n",
        "\n",
        "#Decoder\n",
        "targets = tf.keras.layers.Input(shape=(None,))\n",
        "embedding_layer = tf.keras.layers.Embedding(hindi_vocab_size, d_model, mask_zero=True)\n",
        "x = embedding_layer(targets)\n",
        "decoder_lstm = tf.keras.layers.LSTM(d_model,activation='relu',return_sequences=True, return_state=True)\n",
        "x,_,_ = decoder_lstm(x, initial_state=[state_h, state_c])\n",
        "dense1 = tf.keras.layers.Dense(hindi_vocab_size, activation='softmax')\n",
        "x = dense1(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=[inputs, targets],outputs=x)\n",
        "model.summary()\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 256)    503552      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 256)    523776      input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 256), (None, 525312      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 256),  525312      embedding_3[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 2046)   525822      lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,603,774\n",
            "Trainable params: 2,603,774\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq3TUpTK3TRE"
      },
      "source": [
        "#Create callback to cancel training\n",
        "class TerminateTraining(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if logs['accuracy'] >= 0.95:\n",
        "      self.model.stop_training = True"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxsPj7SL1f-b",
        "outputId": "253f09ec-4144-4fe0-994b-9abaddac09fb"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=epochs, validation_split=0.05, callbacks=[TerminateTraining()])\n",
        "model.save('./drive/MyDrive/ml/models/en-hi.h5')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "  6/706 [..............................] - ETA: 52s - loss: 24.1227 - accuracy: 0.2086WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.1917s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.1917s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "706/706 [==============================] - 55s 73ms/step - loss: 1.0968 - accuracy: 0.2983 - val_loss: 0.6632 - val_accuracy: 0.3621\n",
            "Epoch 2/100\n",
            "706/706 [==============================] - 51s 73ms/step - loss: 0.6317 - accuracy: 0.4534 - val_loss: 0.4546 - val_accuracy: 0.5325\n",
            "Epoch 3/100\n",
            "706/706 [==============================] - 51s 73ms/step - loss: 0.4644 - accuracy: 0.5780 - val_loss: 0.2838 - val_accuracy: 0.6889\n",
            "Epoch 4/100\n",
            "706/706 [==============================] - 52s 73ms/step - loss: 0.3253 - accuracy: 0.6963 - val_loss: 0.1627 - val_accuracy: 0.8134\n",
            "Epoch 5/100\n",
            "706/706 [==============================] - 51s 73ms/step - loss: 0.2237 - accuracy: 0.7882 - val_loss: 0.0995 - val_accuracy: 0.8835\n",
            "Epoch 6/100\n",
            "706/706 [==============================] - 51s 73ms/step - loss: 0.1537 - accuracy: 0.8506 - val_loss: 0.0526 - val_accuracy: 0.9306\n",
            "Epoch 7/100\n",
            "706/706 [==============================] - 52s 73ms/step - loss: 0.1083 - accuracy: 0.8957 - val_loss: 0.0293 - val_accuracy: 0.9586\n",
            "Epoch 8/100\n",
            "706/706 [==============================] - 51s 73ms/step - loss: 0.0833 - accuracy: 0.9226 - val_loss: 0.0214 - val_accuracy: 0.9738\n",
            "Epoch 9/100\n",
            "706/706 [==============================] - 51s 73ms/step - loss: 0.0696 - accuracy: 0.9345 - val_loss: 0.0165 - val_accuracy: 0.9750\n",
            "Epoch 10/100\n",
            "706/706 [==============================] - 52s 74ms/step - loss: 0.0628 - accuracy: 0.9396 - val_loss: 0.0185 - val_accuracy: 0.9718\n",
            "Epoch 11/100\n",
            "706/706 [==============================] - 51s 73ms/step - loss: 0.0565 - accuracy: 0.9432 - val_loss: 0.0124 - val_accuracy: 0.9788\n",
            "Epoch 12/100\n",
            "706/706 [==============================] - 52s 73ms/step - loss: 8.5147 - accuracy: 0.9474 - val_loss: 0.0120 - val_accuracy: 0.9788\n",
            "Epoch 13/100\n",
            "706/706 [==============================] - 52s 73ms/step - loss: 0.0512 - accuracy: 0.9479 - val_loss: 0.0132 - val_accuracy: 0.9756\n",
            "Epoch 14/100\n",
            "706/706 [==============================] - 52s 73ms/step - loss: 0.0524 - accuracy: 0.9492 - val_loss: 0.0123 - val_accuracy: 0.9784\n",
            "Epoch 15/100\n",
            "706/706 [==============================] - 52s 73ms/step - loss: 0.0464 - accuracy: 0.9511 - val_loss: 0.0143 - val_accuracy: 0.9736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBvZo1SRzHbU"
      },
      "source": [
        "#Inference Model\n",
        "\n",
        "#Encoder\n",
        "encoder = tf.keras.models.Model(inputs, [state_h, state_c])\n",
        "\n",
        "#Decoder\n",
        "decoder_input_h = tf.keras.layers.Input(shape=(d_model,))\n",
        "decoder_input_c = tf.keras.layers.Input(shape=(d_model,))\n",
        "x = embedding_layer(targets)\n",
        "x, decoder_output_h, decoder_output_c = decoder_lstm(x, initial_state=[decoder_input_h, decoder_input_c])\n",
        "x = dense1(x)\n",
        "decoder = tf.keras.models.Model([targets] + [decoder_input_h, decoder_input_c], \n",
        "                                [x] + [decoder_output_h, decoder_output_c])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM4WlWIkXbS4"
      },
      "source": [
        "def predict_sentence(en_input):\n",
        "  input_seq = en_tokenizer.texts_to_sequences([en_input])\n",
        "\n",
        "  next_h, next_c = encoder.predict(input_seq)\n",
        "\n",
        "  curr_token = np.zeros(1)\n",
        "  curr_token[0] = hi_tokenizer.word_index['<START>']\n",
        "\n",
        "  pred_sentence = ''\n",
        "\n",
        "  for i in range(maxlen):\n",
        "    output, next_h, next_c = decoder.predict([curr_token] + [next_h, next_c])\n",
        "    next_token = np.argmax(output[0, 0, :])\n",
        "    next_word = hi_tokenizer.index_word[next_token]\n",
        "    if next_word == '<END>':\n",
        "      break\n",
        "    else:\n",
        "      pred_sentence += ' ' + next_word\n",
        "      curr_token[0] = next_token\n",
        "\n",
        "  return pred_sentence"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyPhikavuV5o"
      },
      "source": [
        "#Set up google translate as an additional reference\n",
        "from google.cloud import translate_v2 as translate\n",
        "\n",
        "translate_client = translate.Client.from_service_account_json(\"./drive/MyDrive/gcloud-auth-files/data-shard-330609-2669b6f2c900.json\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq5OesbI4pd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5716f458-8a47-4c9e-e070-534c34569733"
      },
      "source": [
        "#Testing and Analysis\n",
        "import nltk\n",
        "\n",
        "candidates = []\n",
        "references = []\n",
        "\n",
        "for i in range(20):\n",
        "  pred_sentence = predict_sentence(X_test[i])\n",
        "\n",
        "  candidates.append(pred_sentence.split())\n",
        "  google_translated_sentence = translate_client.translate(X_test[i], target_language='hi')['translatedText']\n",
        "  references.append([y_test[i].split()[1:-1], google_translated_sentence.split()])\n",
        "\n",
        "print(nltk.translate.bleu_score.corpus_bleu(references, candidates))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidate:  ['फ़ाइल', 'चुनें']\n",
            "References:  [['फ़ाइल', 'चुनें'], ['फ़ाइल', 'का', 'चयन', 'करें']]\n",
            "0.5210492912666359\n"
          ]
        }
      ]
    }
  ]
}